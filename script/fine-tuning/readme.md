# Fine-tuning using PEFT

## Model that can be used for potential benchmarking
1. GPT-based models (e.g., GPT-3 or GPT-NeoX): Great for generative tasks and general-purpose NLP.
2. T5 (Text-to-Text Transfer Transformer): A more flexible model that treats every NLP task as a text generation task.
3. DeBERTa (Decoding-Enhanced BERT with Disentangled Attention): Improves over BERT in terms of performance and 4. efficiency.
4. RoBERTa (Robustly Optimized BERT): A more powerful version of BERT that is trained on larger data for better performance.
5. BLOOM (BigScience Language Open-science Open-access Multilingual Language Model): A multilingual language model designed to be efficient and powerful.
6. LLaMA (Large Language Model Meta AI): A highly efficient transformer model designed by Meta, great for performance and scalability.
7. Falcon: State-of-the-art models designed by the Technology Innovation Institute, performing exceptionally well on NLP tasks.


## LoRA (Low-Rank Adaptation of Large Language Models)

```
conda activate lora
python lora.py
```

## P-Tuning (Prompt Tuning and P-Tuning v2)


## Adapter Fine-Tuning (Parameter-Efficient Transfer Learning)

## Benchmarking Results of Different Fine-tuning Methods


